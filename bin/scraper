#!/usr/bin/env ruby
# frozen_string_literal: true

# Main class for scrapping https://www.coldwellbankerhomes.com.
#
# For any state or states must be total scraped 20 000 or more products.
#
# Author::    Alexander Petrov mailto:alex.petrofan@gmail.com
# Link::      https://www.linkedin.com/in/alex-petrov/
class Scraper
  # Site homepage.
  BASE_URL = 'https://www.coldwellbankerhomes.com'
  STATES_URL = 'https://www.coldwellbankerhomes.com/sitemap/real-estate/'

  def initialize; end

  def call
    $stdout.puts '== Scraping... =='
    scrap_data
    # export_to_csv
  end

  private

  def get_page_html(url)
    require 'curb'
    require 'nokogiri'

    get_response = Curl.get(url)
    Nokogiri::HTML(get_response.body_str)
  rescue StandardError => error
    $stderr.puts '  -- Error during getting page HTML --'
    $stderr.puts error.inspect
  end

  def scrap_data
    parse_states
    parse_regions
    parse_products
  end

  def parse_states
    $stdout.puts '== Scraping states page... =='
    page_html = get_page_html(STATES_URL)
    return unless page_html

    estates_list = page_html.css('table.table-sort a')
    $stdout.puts estates_list
  end

  # state
  def parse_regions; end

  # region
  def parse_products; end

  def export_to_csv
    require 'csv'

    column_names = %w[street_address state country zip_code price bedrooms
        bathrooms year_built photos url]

    $stdout.puts '== Exporting results to CSV file... =='

    CSV.open('output/products.csv', 'w') do |csv|
      csv << column_names
      csv << %w[data1 data2]
    end
  end
end

Scraper.new.call
