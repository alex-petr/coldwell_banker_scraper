#!/usr/bin/env ruby
# frozen_string_literal: true

# Main class for scrapping https://www.coldwellbankerhomes.com.
#
# For any state or states must be total scraped 20 000 or more products.
#
# Author::    Alexander Petrov mailto:alex.petrofan@gmail.com
# Link::      https://www.linkedin.com/in/alex-petrov/
class Scraper
  require 'curb'
  require 'nokogiri'
  require 'csv'

  # Site homepage.
  BASE_URL = 'https://www.coldwellbankerhomes.com'
  STATES_URL = 'https://www.coldwellbankerhomes.com/sitemap/real-estate/'

  def initialize; end

  def call
    $stdout.puts '== Scraping... =='
    scrap_data
  end

  private

  def get_page_html(url)
    get_response = Curl.get(url)
    Nokogiri::HTML(get_response.body_str)
  rescue StandardError => error
    warn '  -- Error during getting page HTML --'
    warn error.inspect
  end

  # TODO: Refactor this fat method.
  def scrap_data
    # == All states
    states = scrap_states

    $stdout.puts "  -- Scraped total #{states.count} states --"

    states.each do |state|
      $stdout.puts "  #{state[:name].ljust(32)} -> #{state[:url]}"
    end

    export_to_csv('states', states)

    # == 1 Region

    test_state = states.first
    regions = scrap_regions(test_state)

    $stdout.puts "  -- Scraped total #{regions.count} regions --"

    regions.each do |region|
      $stdout.puts "  #{region[:name].ljust(32)} -> #{region[:url]}"
    end

    export_to_csv('regions', regions)

    # == 1 page of products from region

    test_region = regions[1]
    products = scrap_products(test_region)

    $stdout.puts "  -- Scraped total #{products.count} products --"

    products.each do |product|
      $stdout.puts "  #{product[:name].ljust(32)} -> #{product[:url]}"
    end

    export_to_csv("#{test_state[:name].downcase.tr(' ', '_')}_"\
      "#{test_region[:name].downcase.tr(' ', '_')}_products", products)
  end

  def scrap_states
    $stdout.puts "\n== Scraping states page =="
    page_html = get_page_html(STATES_URL)
    return unless page_html

    estates_list = page_html.css('table.table-sort a')
    return unless estates_list

    estates_list.map do |estate_node|
      { name: estate_node.content, url: BASE_URL + estate_node[:href] }
    end
  end

  def scrap_regions(state)
    $stdout.puts "\n== Scraping state `#{state[:name]}` page =="

    page_html = get_page_html(state[:url])
    return unless page_html

    regions_list = page_html.css('table.table-sort a')
    return unless regions_list

    regions_list.map do |estate_node|
      { name: estate_node.content, url: BASE_URL + estate_node[:href] }
    end
  end

  def scrap_products(region)
    $stdout.puts "\n== Scraping region `#{region[:name]}` page =="

    page_html = get_page_html(region[:url])
    return unless page_html

    products_list = page_html.css('div#searchList div.prop-info div.address a')
    return unless products_list

    products_list.map do |estate_node|
      { name: estate_node.at_css('.street-address').content
                         .gsub(/[[:space:]]/, ' ').strip,
        url: BASE_URL + estate_node[:href] }
    end
  end

  # TODO: Add implementation, `url` param.
  def scrap_product; end

  def export_to_csv(name, data)
    file_name = output_csv_filename(name)

    remove_existing_file(file_name)

    CSV.open(file_name, 'w') do |csv|
      csv << data.first.keys
      data.each { |record| csv << record.each_value }
    end

    csv_file_created?(file_name)
  rescue StandardError => error
    warn '  -- Error during saving to CSV file --'
    warn error.inspect
  end

  def output_csv_filename(file_name)
    $stdout.puts "\n== Exporting #{file_name} to CSV file =="
    "output/#{file_name}.csv"
  end

  def remove_existing_file(file_name)
    File.delete(file_name) if File.exist?(file_name)
  end

  def csv_file_created?(file_name)
    return unless File.exist?(file_name)
    $stdout.puts "  -> File `#{file_name}` successfully created :D --"
  end
end

Scraper.new.call
