#!/usr/bin/env ruby
# frozen_string_literal: true

# Main class for scrapping https://www.coldwellbankerhomes.com.
#
# For any state or states must be total scraped 20 000 or more products.
#
# Author::    Alexander Petrov mailto:alex.petrofan@gmail.com
# Link::      https://www.linkedin.com/in/alex-petrov/
class Scraper
  require 'curb'
  require 'nokogiri'
  require 'csv'

  # Site homepage.
  BASE_URL = 'https://www.coldwellbankerhomes.com'

  def initialize; end

  def call
    $stdout.puts '== Scraping... =='
    scrap_data
  end

  private

  def get_page_html(url)
    get_response = Curl.get(url)
    Nokogiri::HTML(get_response.body_str)
  rescue StandardError => error
    warn '  -- Error during getting page HTML --'
    warn error.inspect
  end

  def scrap_data
    # TODO: Single thread scraping.
    # scrap_states.each do |state|
    #   scrape_regions(state).each { |reg| scrape_region_products(state, reg) }
    # end

    # == All states
    states = scrap_states
    state = states.first

    # == 1 Region
    regions = scrape_regions(state)
    region = regions[1]

    # == 1 page of products from region
    scrape_region_products(state, region)
  end

  def scrap_states
    states = scrap_links_page('states', "#{BASE_URL}/sitemap/real-estate/",
                              'table.table-sort a')

    $stdout.puts "  -- Scraped total #{states.count} states --"
    print_name_link(states)

    export_to_csv('1_states', states)

    states
  end

  def scrape_regions(state)
    regions = scrap_links_page("state `#{state[:name]}`",
                               state[:url], 'table.table-sort a')

    $stdout.puts "  -- Scraped total #{regions.count} regions --"
    print_name_link(regions)

    export_to_csv("2_#{state[:name]}_regions", regions)

    regions
  end

  def scrape_region_products(state, region)
    products = scrap_links_page("region `#{region[:name]}`",
                                region[:url],
                                'div#searchList div.prop-info div.address a',
                                '.street-address')

    $stdout.puts "  -- Scraped total #{products.count} products --"
    print_name_link(products)

    export_to_csv("3_#{state[:name]}_#{region[:name]}_products", products)

    products
  end

  def scrap_links_page(name, url, selector, name_selector = nil)
    $stdout.puts "\n== Scraping #{name} page =="

    # Performance note: condition inside loop is bad, but for this case its ok.
    get_page_html(url)&.css(selector)&.map do |link_node|
      link_name_node = link_node
      link_name_node = link_node.at_css(name_selector) if name_selector

      { name: link_name_node.content.gsub(/[[:space:]]/, ' ').strip,
        url: BASE_URL + link_node[:href] }
    end
  end

  # TODO: Add implementation, `url` param.
  def scrap_product; end

  def print_name_link(name_links)
    name_links.each do |name_link|
      $stdout.puts "  #{name_link[:name].ljust(32)} -> #{name_link[:url]}"
    end
  end

  def export_to_csv(name, data)
    file_name = output_csv_filename(name)

    remove_existing_file(file_name)

    CSV.open(file_name, 'w') do |csv|
      csv << data.first.keys
      data.each { |record| csv << record.each_value }
    end

    csv_file_created?(file_name)
  rescue StandardError => error
    warn '  -- Error during saving to CSV file --'
    warn error.inspect
  end

  def output_csv_filename(file_name)
    file_name = file_name.downcase.tr(' ', '_')
    $stdout.puts "\n== Exporting `#{file_name}` ->> CSV file =="
    "output/#{file_name}.csv"
  end

  def remove_existing_file(file_name)
    File.delete(file_name) if File.exist?(file_name)
  end

  def csv_file_created?(file_name)
    return unless File.exist?(file_name)
    $stdout.puts "  -> File `#{file_name}` successfully created :D --"
  end
end

Scraper.new.call
